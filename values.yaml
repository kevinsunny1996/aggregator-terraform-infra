# #----------------------------------------------------
# # Security context for airflow
# #----------------------------------------------------
# securityContext:
#   fsGroup: 65534

# airflowVersion: "2.5.0"

# executor: "KubernetesExecutor"

# #----------------------------------------------------
# # Ingress configuration with AWS LB Controller
# # Checkout this doc for more annotations https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/
# #----------------------------------------------------
# ingress:
#   web:
#     enabled: true
#     annotations:
#       alb.ingress.kubernetes.io/group.name: dataengineering
#       alb.ingress.kubernetes.io/target-type: instance
#       alb.ingress.kubernetes.io/scheme: internet-facing
#       alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
#       alb.ingress.kubernetes.io/healthcheck-path: '/health'
#       # Enable the following if you have public/internal domain e.g., https://mycompany.com/
#       # alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS": 443}]'
#       # alb.ingress.kubernetes.io/certificate-arn: "arn:aws:acm:....................."

#     path: '/'
#     # PathType for the above path (to be used for K8s 1.19 and above)
#     pathType: "Prefix"
#     # Hostnames or hosts to be used in the configguration
#     hosts:
#       - name: ""
#         tls:
#           # Enable TLS termination for this web ingress , once the HTTPS URL is created
#           enabled: false
#           # Name of precreated secret key containing TLS private key and certificate
#           secretName: ""
#     ingressClassName: alb

# # ---------------------------------------------------------------------------------------------
# # Airflow DB
# # ---------------------------------------------------------------------------------------------
# data:
#   metadataConnection:
#     user: ${airflow_db_user}
#     pass:
#     protocol: postgresql
#     host: ${airflow_db_host}
#     port: 5432
#     db: ${airflow_db_name}
#     sslmode: disable

# # ---------------------------------------------------------------------------------------------
# # Airflow Worker config
# # ---------------------------------------------------------------------------------------------
# workers:
#   serviceAccount: 
#     create: false
#     name: ${airflow_service_account}
#   persistence:
#     enabled: false
#   resources:
#     limits:
#       cpu: 200m
#       memory: 256Mi
#     requests:
#       cpu: 200m
#       memory: 256Mi

# # ----------------------------------------------------------------------------------------------
# # Airflow scheduler settings
# # ----------------------------------------------------------------------------------------------
# scheduler:
#   replicas: 2
#   serviceAccount:
#     create: false
#     name: ${airflow_service_account}
#   resources:
#     limits:
#       cpu: 200m
#       memory: 512Mi
#     requests:
#       cpu: 200m
#       memory: 512Mi

# # ----------------------------------------------------------------------------------------------
# # Airflow Database migration settings
# # ----------------------------------------------------------------------------------------------
# migrateDatabaseJob:
#   enabled: true
#   command: ~
#   args:
#    - "bash"
#    - "-c"
#    - |-
#     exec \
#     airflow {{ semverCompare ">=2.0.0" .Values.airflowVersion | ternary "db upgrade" "upgradedb" }}


# # ----------------------------------------------------------------------------------------------
# # Airflow Webserver settings
# # ----------------------------------------------------------------------------------------------
# webserverSecretKeySecretName: ${webserver_secret_name}

# webserver:
#   replicas: 2
#   serviceAccount:
#     create: false
#     name: ${airflow_service_account}
#   resources:
#     limits:
#       cpu: 200m
#       memory: 1Gi
#     requests:
#       cpu: 200m
#       memory: 1Gi
#   allowPodLogReading: true
#   livenessProbe:
#     initialDelaySeconds: 15
#     timeoutSeconds: 30
#     failureThreshold: 20
#     periodSeconds: 5

#   readinessProbe:
#     initialDelaySeconds: 15
#     timeoutSeconds: 30
#     failureThreshold: 20
#     periodSeconds: 5

# # Configuring Ingress to Airflow UI hence the service type is changed to NodePort
#   service:
#     type: NodePort
#     ports: 
#     - name: airflow-ui
#       port: "{{ .Values.ports.airflowUI }}" 

# #----------------------------------------------------
# # Airflow Triggerer Config
# #----------------------------------------------------
# triggerer:
#   enabled: true

# #----------------------------------------------------
# # Airflow Dag Processor Config
# #----------------------------------------------------
# dagProcessor:
#   enabled: true

# #----------------------------------------------------
# # Airflow StatsD
# #----------------------------------------------------
# statsd:
#   enabled: true
#   resources:
#     limits:
#       cpu: 100m
#       memory: 128Mi
#     requests:
#       cpu: 100m
#       memory: 128Mi

# #----------------------------------------------------
# # Airflow PgBouncer
# #----------------------------------------------------
# pgbouncer:
#   enabled: true
#   auth_type: scram-sha-256

# #----------------------------------------------------
# # Disable local execution of Postgres for external RDS
# #----------------------------------------------------
# postgresql:
#   enabled: false

# #----------------------------------------------------
# # Config for remote S3 logging
# #----------------------------------------------------
# config:
#   core:
#     dags_folder: '{{ include "src" . }}'
#     load_examples: false
#     executor: '{{ .Values.executor }}'
#     colored_console_log: 'True'
#     remote_logging: 'True'

#     # Logging configured to remote S3 bucket
#     logging:
#       remote_logging: 'True'
#       logging_level: 'INFO'
#       colored_console_log: 'True'
#       remote_base_log_folder: "s3://${s3_bucket_name}/airflow-logs"
      
#       # aws_s3_conn is the name of the connection that needs to be created using Airflow admin UI once the deployment is complete
#       # Steps can be seen in the docs link here -> https://github.com/apache/airflow/issues/25322
#       remote_log_conn_id: 'aws_s3_conn'
#       # Keep the workers used by K8s Executor up after completion?
#       delete_worker_pods: 'True'
#       encrypt_s3_logs: 'True'
#     metrics:
#       statsd_on: '{{ ternary "True" "False" .Values.statsd.enabled }}'
#       statsd_port: 9125
#       statsd_prefix: airflow
#       statsd_host: '{{ printf "%s-statsd" .Release.Name }}'
#       run_duration: 120
#     kubernetes:
#       namespace: '{{ .Release.Namespace }}'
#       airflow_configmap: '{{ include "airflow_config" . }}'
#       airflow_local_settings_configmap: '{{ include "airflow_config" . }}'
#       pod_template_file: '{{ include "airflow_pod_template_file" . }}/pod_template_file.yaml'
#       worker_container_repository: '{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}'
#       worker_container_tag: '{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}'
#       multi_namespace_mode: '{{ ternary "True" "False" .Values.multiNamespaceMode }}'

# #----------------------------------------------------
# # Git sync
# #----------------------------------------------------
# # Mounting DAGs using Git-Sync sidecar with Persistence enabled with EFS
# # This option will use a EFS Persistent Volume Claim with an access mode of ReadWriteMany.
# # The scheduler pod will sync DAGs from a git repository onto the PVC every configured number of seconds. The other pods will read the synced DAGs.
# dags:
#   persistence:
#     enabled: true
#     size: 5Gi
#     storageClassName: efs-sc
#     accessMode: ReadWriteMany
#     existingClaim: ${efs_pvc}
  
#   # This example using a sample airflow-dags repo(airflow-dags.git) to demonstrate the GitSync Feature
#   # You can replace this with your own internal private repo and provide subPath for your DAGS folder
#   # Multiple folders can be created for each sub tenant under DAGS folder
#   gitSync:
#     enabled: true
#     repo: git@github.com:kevinsunny1996/Aggregator-Data-Pipeline.git
#     branch: main
#     rev: HEAD
#     depth: 2
#     maxFailures: 0
#     subPath: "dags"
#     sshKeySecret: ""
#     # This is mandatory for gitSync feature
#     # Checkout the docs for creating knownHosts key for Github https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#knownhosts
#     knownHosts: | 
#       github.com ssh-rsa 
#       AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==

#     # Adjust the resources according to your workloads
#     resources:
#       limits:
#         cpu: 100m
#         memory: 128Mi
#       requests:
#         cpu: 100m
#         memory: 128Mi

    


